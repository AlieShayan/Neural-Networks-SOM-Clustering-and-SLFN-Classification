{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65bfdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and basic setup completed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy # For deep copying weights in early stopping\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler # Added MinMaxScaler for experimentation suggestion\n",
    "from minisom import MiniSom\n",
    "np.random.seed(42)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Imports and basic setup completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2c21c",
   "metadata": {},
   "source": [
    "## SLFN for Titanic Classification - Data Loading and Preprocessing\n",
    "\n",
    "Load the Titanic dataset (downloading if necessary) and define/apply the preprocessing steps (feature engineering, imputation, encoding, scaling). Split into final train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31402bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Task 3: SLFN for Titanic Classification ---\n",
      "Titanic dataset loaded successfully from local file: 'titanic.csv'.\n",
      "\n",
      "Preprocessing Titanic data...\n",
      "Filled missing Embarked values with mode: 'S'\n",
      "Dropped columns: ['Name', 'Ticket', 'Cabin', 'PassengerId', 'SibSp', 'Parch']\n",
      "Features used for modeling: ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'FamilySize', 'IsAlone']\n",
      "Preprocessing complete.\n",
      "Scaled features shape (X): (891, 8)\n",
      "Target labels shape (y): (891,)\n",
      "\n",
      "Titanic data split:\n",
      "  Final Train set: 534 samples\n",
      "  Validation set:  178 samples\n",
      "  Test set:        179 samples\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Task 3: SLFN for Titanic Classification ---\")\n",
    "# --- Load Titanic Data (Robust Loading) ---\n",
    "titanic_file = 'titanic.csv'\n",
    "df_titanic = None\n",
    "data_source_url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "\n",
    "# Try loading locally first\n",
    "if os.path.exists(titanic_file):\n",
    "    try:\n",
    "        df_titanic = pd.read_csv(titanic_file)\n",
    "        print(f\"Titanic dataset loaded successfully from local file: '{titanic_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading local titanic.csv: {e}. Will attempt download.\")\n",
    "        df_titanic = None # Ensure it's None if local read fails\n",
    "# Titanic Preprocessing Function\n",
    "def preprocess_titanic(df):\n",
    "    \"\"\"Preprocesses the Titanic DataFrame for SLFN classification.\"\"\"\n",
    "    print(\"\\nPreprocessing Titanic data...\")\n",
    "    if df is None:\n",
    "        print(\"Error: Input DataFrame is None. Cannot preprocess.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    data = df.copy() # Work on a copy\n",
    "\n",
    "    # Extract Title from Name\n",
    "    data['Title'] = data['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "    # Consolidate rare titles\n",
    "    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    data['Title'] = data['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n",
    "    # Map Title to numerical values\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    data['Title'] = data['Title'].map(title_mapping)\n",
    "    data['Title'] = data['Title'].fillna(0) # Fill any NaNs in Title (e.g., if regex failed)\n",
    "\n",
    "    # Create FamilySize\n",
    "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
    "    # Create IsAlone feature\n",
    "    data['IsAlone'] = 0\n",
    "    data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "    # Handling Missing Values: Age, Embarked, Fare\n",
    "    # Impute Age based on median age per Title group\n",
    "    try:\n",
    "      data['Age'] = data.groupby('Title')['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "    except KeyError as e:\n",
    "       print(f\"Warning: Groupby operation failed, likely due to missing 'Title' mapping for some rows. Check Title extraction/mapping. Error: {e}\")\n",
    "    # If any Age NaNs remain (e.g., a Title group had all NaNs), fill with global median Age\n",
    "    if data['Age'].isnull().any():\n",
    "        data['Age'] = data['Age'].fillna(data['Age'].median())\n",
    "\n",
    "    # Impute Embarked with the mode\n",
    "    if data['Embarked'].isnull().any():\n",
    "        mode_embarked = data['Embarked'].mode()[0]\n",
    "        data['Embarked'] = data['Embarked'].fillna(mode_embarked)\n",
    "        print(f\"Filled missing Embarked values with mode: '{mode_embarked}'\")\n",
    "\n",
    "    # Impute Fare with the median Fare\n",
    "    if data['Fare'].isnull().any():\n",
    "        median_fare = data['Fare'].median()\n",
    "        data['Fare'] = data['Fare'].fillna(median_fare)\n",
    "        print(f\"Filled missing Fare values with median: {median_fare:.2f}\")\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    columns_to_drop = ['Name', 'Ticket', 'Cabin', 'PassengerId', 'SibSp', 'Parch']\n",
    "    # Check if columns exist before dropping\n",
    "    columns_exist = [col for col in columns_to_drop if col in data.columns]\n",
    "    if columns_exist:\n",
    "      data = data.drop(columns_exist, axis=1)\n",
    "      print(f\"Dropped columns: {columns_exist}\")\n",
    "\n",
    "\n",
    "    # Convert Categorical Features to Numerical: Sex, Embarked\n",
    "    data['Sex'] = data['Sex'].map({'male': 0, 'female': 1}).astype(int)\n",
    "    data['Embarked'] = data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
    "\n",
    "    # Final check for any remaining NaNs (e.g., if median imputation failed)\n",
    "    if data.isnull().sum().sum() > 0:\n",
    "        print(f\"Warning: Found {data.isnull().sum().sum()} remaining NaN values after initial imputation. Filling with column medians.\")\n",
    "        data = data.fillna(data.median()) # Fill any remaining NaNs with column medians\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    if 'Survived' not in data.columns:\n",
    "        print(\"Error: 'Survived' column not found in the DataFrame.\")\n",
    "        return None, None, None, None\n",
    "    X = data.drop('Survived', axis=1)\n",
    "    y = data['Survived']\n",
    "    feature_names = list(X.columns)\n",
    "    print(f\"Features used for modeling: {feature_names}\")\n",
    "\n",
    "    # Scale numerical features\n",
    "    feature_scaler = StandardScaler()\n",
    "    X_scaled = feature_scaler.fit_transform(X)\n",
    "\n",
    "    # Convert y to NumPy array\n",
    "    y = y.values\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "    print(f\"Scaled features shape (X): {X_scaled.shape}\")\n",
    "    print(f\"Target labels shape (y): {y.shape}\")\n",
    "    return X_scaled, y, feature_scaler, feature_names\n",
    "\n",
    "# Preprocess and Split Titanic Data\n",
    "titanic_data_available = False\n",
    "if df_titanic is not None:\n",
    "    X_titanic_scaled, y_titanic, titanic_scaler, titanic_features = preprocess_titanic(df_titanic)\n",
    "\n",
    "    if X_titanic_scaled is not None and y_titanic is not None:\n",
    "        titanic_data_available = True\n",
    "        # Split Titanic data: Initial Train -> Test, then Initial Train -> Final Train + Validation\n",
    "        # Using similar 60/20/20 split as for digits\n",
    "        X_titanic_train_init, X_titanic_test, y_titanic_train_init, y_titanic_test = train_test_split(\n",
    "            X_titanic_scaled, y_titanic,\n",
    "            test_size=0.2, random_state=42, stratify=y_titanic\n",
    "        )\n",
    "        X_titanic_train_final, X_titanic_val, y_titanic_train_final, y_titanic_val = train_test_split(\n",
    "            X_titanic_train_init, y_titanic_train_init,\n",
    "            test_size=0.25, # 0.25 * 0.8 = 0.2 -> 20% validation set\n",
    "            random_state=42, stratify=y_titanic_train_init\n",
    "        )\n",
    "        print(f\"\\nTitanic data split:\")\n",
    "        print(f\"  Final Train set: {X_titanic_train_final.shape[0]} samples\")\n",
    "        print(f\"  Validation set:  {X_titanic_val.shape[0]} samples\")\n",
    "        print(f\"  Test set:        {X_titanic_test.shape[0]} samples\")\n",
    "        print(\"-\" * 30)\n",
    "    else:\n",
    "        print(\"\\nSkipping Titanic SLFN task due to preprocessing failure.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Titanic preprocessing and SLFN task due to data loading failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc52a1b",
   "metadata": {},
   "source": [
    "## SLFN Implementation (From Scratch - Fixed Epochs)\n",
    "\n",
    "Define the Single Layer Feedforward Network (SLFN) class using only NumPy for core operations. This version runs for a **fixed number of epochs** as requested, removing the early stopping logic. It still includes momentum and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a79aecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLFN class defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# Implement SLFN from scratch\n",
    "if titanic_data_available:\n",
    "    class SLFN:\n",
    "        def __init__(self, input_size, hidden_size, output_size=1, learning_rate=0.01,\n",
    "                     momentum=0.9, l2_lambda=0.01):\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.output_size = output_size\n",
    "            self.learning_rate = learning_rate\n",
    "            self.momentum = momentum\n",
    "            self.l2_lambda = l2_lambda # L2 regularization strength\n",
    "\n",
    "            # Initialize random weights and biases\n",
    "            self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "            self.b1 = np.zeros((1, hidden_size))\n",
    "\n",
    "            self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
    "            self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "            # Initialize velocities for Momentum optimizer\n",
    "            self.v_W1 = np.zeros_like(self.W1)\n",
    "            self.v_b1 = np.zeros_like(self.b1)\n",
    "            self.v_W2 = np.zeros_like(self.W2)\n",
    "            self.v_b2 = np.zeros_like(self.b2)\n",
    "\n",
    "            # Placeholders for intermediate values during forward/backward pass\n",
    "            self.z1, self.a1, self.z2, self.a2 = None, None, None, None\n",
    "\n",
    "            # Lists to store training history\n",
    "            self.train_losses = []\n",
    "            self.val_losses = [] # Track validation loss for early stopping\n",
    "            print(f\"SLFN initialized:\")\n",
    "            print(f\"  Layers: Input({input_size}) -> Hidden({hidden_size}) -> Output({output_size})\")\n",
    "            print(f\"  Hyperparameters: LR={learning_rate}, Momentum={momentum}, L2 Lambda={l2_lambda}\")\n",
    "\n",
    "        # --- Activation Functions -----------------\n",
    "        def relu(self, x):\n",
    "            \"\"\"ReLU activation function.\"\"\"\n",
    "            return np.maximum(0, x)\n",
    "\n",
    "        def relu_derivative(self, x):\n",
    "            \"\"\"Derivative of ReLU activation function.\"\"\"\n",
    "            return np.where(x > 0, 1, 0)\n",
    "\n",
    "        def sigmoid(self, x):\n",
    "            # Clip input to avoid overflow in exp(-x) for large negative x\n",
    "            clipped_x = np.clip(x, -500, 500)\n",
    "            return 1 / (1 + np.exp(-clipped_x))\n",
    "\n",
    "        # Implement the forward pass\n",
    "        def forward(self, X):\n",
    "            # Ensure X is a 2D array (batch_size, n_features)\n",
    "            if not isinstance(X, np.ndarray): X = np.array(X)\n",
    "            if X.ndim == 1: X = X.reshape(1, -1) # Handle single sample prediction\n",
    "\n",
    "            # Layer 1 (Input to Hidden)\n",
    "            self.z1 = np.dot(X, self.W1) + self.b1\n",
    "            # Step 2.b (cont.): Use ReLU activation for the hidden layer\n",
    "            self.a1 = self.relu(self.z1)\n",
    "\n",
    "            # Layer 2 (Hidden to Output)\n",
    "            self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "            # Step 2.c: Implement the sigmoid output layer for classification\n",
    "            self.a2 = self.sigmoid(self.z2) # Output probabilities\n",
    "\n",
    "            return self.a2\n",
    "\n",
    "        # Step 2.d: Compute cross-entropy loss\n",
    "        def compute_loss(self, y_true, y_pred, include_l2=False):\n",
    "            if not isinstance(y_true, np.ndarray): y_true = np.array(y_true)\n",
    "            # Ensure y_true has the same shape as y_pred (N, 1)\n",
    "            y_true = y_true.reshape(y_pred.shape)\n",
    "            m = y_true.shape[0] # Number of samples in the batch\n",
    "            epsilon = 1e-9 # Small value to avoid log(0)\n",
    "\n",
    "            # Clip predictions to avoid log(0) or log(1) issues\n",
    "            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "            # Binary Cross-Entropy\n",
    "            cross_entropy_term = - (y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "            cross_entropy_loss = np.mean(cross_entropy_term)\n",
    "            l2_penalty = 0\n",
    "            if include_l2 and self.l2_lambda > 0:\n",
    "                # Factor of 1/m instead of 1/(2*m) because loss is already averaged\n",
    "                l2_penalty = (self.l2_lambda / 2) * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))\n",
    "\n",
    "            return cross_entropy_loss + l2_penalty\n",
    "\n",
    "        # --- Step 2.e: Implement backpropagation ---\n",
    "        def backward(self, X, y):\n",
    "            # Ensure inputs are NumPy arrays and correctly shaped\n",
    "            if not isinstance(X, np.ndarray): X = np.array(X)\n",
    "            if X.ndim == 1: X = X.reshape(1, -1)\n",
    "            if not isinstance(y, np.ndarray): y = np.array(y)\n",
    "            y = y.reshape(self.a2.shape) # Ensure y is (m, 1)\n",
    "            m = X.shape[0] # Batch size\n",
    "\n",
    "            # --- Calculate Gradients ---\n",
    "            dz2 = (self.a2 - y) / m\n",
    "\n",
    "            dW2 = np.dot(self.a1.T, dz2) + (self.l2_lambda / m) * self.W2\n",
    "            db2 = np.sum(dz2, axis=0, keepdims=True) # Sum gradients over the batch for bias\n",
    "\n",
    "            da1 = np.dot(dz2, self.W2.T)\n",
    "\n",
    "            dz1 = da1 * self.relu_derivative(self.z1)\n",
    "\n",
    "            # Add L2 regularization gradient term for weights (lambda/m * W)\n",
    "            dW1 = np.dot(X.T, dz1) + (self.l2_lambda / m) * self.W1\n",
    "            db1 = np.sum(dz1, axis=0, keepdims=True) # Sum gradients over the batch for bias\n",
    "\n",
    "            # Update velocities\n",
    "            self.v_W1 = self.momentum * self.v_W1 + self.learning_rate * dW1\n",
    "            self.v_b1 = self.momentum * self.v_b1 + self.learning_rate * db1\n",
    "            self.v_W2 = self.momentum * self.v_W2 + self.learning_rate * dW2\n",
    "            self.v_b2 = self.momentum * self.v_b2 + self.learning_rate * db2\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.W1 -= self.v_W1\n",
    "            self.b1 -= self.v_b1\n",
    "            self.W2 -= self.v_W2\n",
    "            self.b2 -= self.v_b2\n",
    "\n",
    "        def train(self, X_train, y_train, X_val, y_val, epochs=1000, batch_size=32,\n",
    "                  patience=10, verbose=True, print_every=100, seed=None):\n",
    "            # Set seed for reproducible shuffling\n",
    "            if seed is not None:\n",
    "                np.random.seed(seed)\n",
    "\n",
    "\n",
    "            print(f\"\\nStarting SLFN training:\")\n",
    "            print(f\"  Epochs: {epochs}, Batch Size: {batch_size}, Early Stopping Patience: {patience}, Seed: {seed}\")\n",
    "            if not isinstance(X_train, np.ndarray): X_train = np.array(X_train)\n",
    "            if not isinstance(y_train, np.ndarray): y_train = np.array(y_train)\n",
    "            m = X_train.shape[0]\n",
    "            self.train_losses = []\n",
    "            self.val_losses = []\n",
    "\n",
    "            best_val_loss = np.inf\n",
    "            epochs_no_improve = 0\n",
    "            best_weights = None\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                epoch_train_losses = []\n",
    "                indices = np.random.permutation(m)\n",
    "                X_shuffled = X_train[indices]\n",
    "                y_shuffled = y_train[indices]\n",
    "\n",
    "                # Mini-batch training loop\n",
    "                for i in range(0, m, batch_size):\n",
    "                    # Get mini-batch\n",
    "                    X_batch = X_shuffled[i:min(i + batch_size, m)]\n",
    "                    y_batch = y_shuffled[i:min(i + batch_size, m)]\n",
    "\n",
    "                    y_pred_batch = self.forward(X_batch)\n",
    "\n",
    "                    batch_loss = self.compute_loss(y_batch, y_pred_batch, include_l2=True)\n",
    "                    epoch_train_losses.append(batch_loss)\n",
    "\n",
    "                    self.backward(X_batch, y_batch)\n",
    "\n",
    "                # Record average training loss for the epoch\n",
    "                avg_epoch_train_loss = np.mean(epoch_train_losses)\n",
    "                self.train_losses.append(avg_epoch_train_loss)\n",
    "\n",
    "                # Validation Step for Early Stopping---------\n",
    "                # Calculate loss on the validation set (without L2 penalty for fair comparison)\n",
    "                y_pred_val = self.forward(X_val)\n",
    "                current_val_loss = self.compute_loss(y_val, y_pred_val, include_l2=False)\n",
    "                self.val_losses.append(current_val_loss)\n",
    "\n",
    "                # Calculate Train/Val Accuracy for monitoring progress\n",
    "                train_acc, val_acc = 0.0, 0.0\n",
    "                if verbose and (epoch % print_every == 0 or epoch == epochs - 1):\n",
    "                     train_preds = (self.predict_proba(X_train) >= 0.5).astype(int).flatten()\n",
    "                     val_preds = (y_pred_val >= 0.5).astype(int).flatten()\n",
    "                     train_acc = np.mean(train_preds == y_train.flatten())\n",
    "                     val_acc = np.mean(val_preds == y_val.flatten())\n",
    "                     print(f\"Epoch {epoch+1:5d}/{epochs} | Train Loss: {avg_epoch_train_loss:.6f} | Val Loss: {current_val_loss:.6f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "                # Check for improvement in validation loss\n",
    "                if current_val_loss < best_val_loss:\n",
    "                    best_val_loss = current_val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    # Save the best weights found so far (use deepcopy)\n",
    "                    best_weights = {\n",
    "                        'W1': copy.deepcopy(self.W1), 'b1': copy.deepcopy(self.b1),\n",
    "                        'W2': copy.deepcopy(self.W2), 'b2': copy.deepcopy(self.b2),\n",
    "                        'v_W1': copy.deepcopy(self.v_W1), 'v_b1': copy.deepcopy(self.v_b1),\n",
    "                        'v_W2': copy.deepcopy(self.v_W2), 'v_b2': copy.deepcopy(self.v_b2)\n",
    "                    }\n",
    "                    if verbose and epoch > patience: # Only print if improvement happens after initial phase\n",
    "                       print(f\"  (Improvement found! Best Val Loss: {best_val_loss:.6f})\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                # Trigger early stopping if validation loss hasn't improved for 'patience' epochs\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered after {epoch+1} epochs.\")\n",
    "                    print(f\"No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "                    print(f\"Best Validation Loss achieved: {best_val_loss:.6f}\")\n",
    "                    break # Exit training loop\n",
    "\n",
    "            print(\"Training complete.\")\n",
    "            # Restore the best weights found during training based on validation loss\n",
    "            if best_weights:\n",
    "                print(\"Restoring best weights based on validation performance.\")\n",
    "                self.W1, self.b1 = best_weights['W1'], best_weights['b1']\n",
    "                self.W2, self.b2 = best_weights['W2'], best_weights['b2']\n",
    "                self.v_W1, self.v_b1 = best_weights['v_W1'], best_weights['v_b1']\n",
    "                self.v_W2, self.v_b2 = best_weights['v_W2'], best_weights['v_b2']\n",
    "            else:\n",
    "                print(\"Warning: No best weights saved (training might have been too short or parameters unstable). Using final weights.\")\n",
    "\n",
    "\n",
    "        # --- Prediction Functions ---\n",
    "        def predict_proba(self, X):\n",
    "            \"\"\"Predicts class probabilities (output of sigmoid).\"\"\"\n",
    "            return self.forward(X)\n",
    "\n",
    "        def predict(self, X, threshold=0.5):\n",
    "             \"\"\"Predicts class labels (0 or 1) based on the probability threshold.\"\"\"\n",
    "             probabilities = self.predict_proba(X)\n",
    "             # Return 1 if probability >= threshold, else 0\n",
    "             return (probabilities >= threshold).astype(int)\n",
    "\n",
    "    print(\"SLFN class defined successfully.\")\n",
    "else:\n",
    "    print(\"SLFN class definition skipped as Titanic data is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa83df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
